{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [\n",
    "\t\t\t\"2 seater sofa\", \"bench\", \"book cases\",\n",
    "\t\t\t\"coffee table\", \"dining set\",\"queen beds\",\n",
    "\t\t\t\"arm chairs\",\"chest drawers\",\"garden seating\",\n",
    "\t\t\t\"bean bags\",\"king beds\"\n",
    "\t\t]\n",
    "\n",
    "urls = []\n",
    "dir_names = []\n",
    "\n",
    "base_url = \"https://www.pepperfry.com/site_product/search?q=\"\n",
    "\n",
    "base_dir = \"./pepperfry-data\"\n",
    "if not os.path.exists(base_dir):\n",
    "  os.mkdir(base_dir)\n",
    "base_path = \"./pepperfry-data/\"\n",
    "\n",
    "for item in items:\n",
    "  query_str = \"+\".join(item.split())\n",
    "  dir_name = \"-\".join(item.split())\n",
    "\n",
    "  urls.append(base_url+query_str)\n",
    "  dir_path = base_path+dir_name\n",
    "  dir_names.append(dir_path)\n",
    "  if not os.path.exists(dir_path):\n",
    "    os.mkdir(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "48\n",
      "48\n",
      "48\n",
      "47\n",
      "46\n",
      "47\n",
      "48\n",
      "47\n",
      "48\n",
      "48\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "for i in range(len(urls)):\n",
    "  req = requests.get(urls[i], headers=headers)\n",
    "  soup = bs(req.content)\n",
    "  product_urls = soup.findAll('div', {'class': 'card-img-wrp center-xs card-srch-img-wrp'})\n",
    "  # print(len(product_urls))\n",
    "  for k in range(20):\n",
    "    url = product_urls[k]\n",
    "    product_page_url = url.a.attrs['href']\n",
    "    product_page = requests.get(product_page_url, headers = headers)\n",
    "    product_page_soup = bs(product_page.content)\n",
    "    product_title = product_page_soup.find('h1', {'class':'v-pro-ttl pf-medium-bold-text'})\n",
    "    product_dir = \"-\".join(product_title.text.split())\n",
    "    dir_name = dir_names[i] + \"/\" + product_dir\n",
    "    if not os.path.exists(dir_name):\n",
    "      os.mkdir(dir_name)\n",
    "    # print(product_title)\n",
    "    product_img = product_page_soup.findAll('li', {'class':'vipImage__thumb-each noClickSlide'})\n",
    "    for temp in range(len(product_img)): # enumerate to get the index\n",
    "      if product_img[temp].find('a', {'class': 'vipImage__thumb-3d'}) == None:\n",
    "        with open(dir_name +'/img{}.jpg'.format(temp), 'wb') as file:            \n",
    "            img_url = product_img[temp].a.attrs[\"data-hoverimg\"]\n",
    "            response = requests.get(img_url)\n",
    "            file.write(response.content)\n",
    "    \n",
    "    product_price = product_page_soup.findAll('div', {'itemprop': 'offers'})\n",
    "    if len(product_price) >= 1:\n",
    "      price = product_price[0].meta.attrs['content']\n",
    "    else:\n",
    "      price = product_page_soup.find('span', {'class': 'v-price-mrp-amt-only'})[\"data-price\"]\n",
    "\n",
    "    product_savings = product_page_soup.findAll('span', {'class': 'v-price-save-ttl-amt v-price-col-right total_saving'})\n",
    "    savings = \"\"\n",
    "    \n",
    "    if len(savings) >= 1:\n",
    "      savings = product_savings[0].text\n",
    "    else:\n",
    "      savings = \"0\"\n",
    "    # print(savings)\n",
    "\n",
    "    product_description = product_page_soup.find(['div', 'p'], {'class': 'v-more-info-tab-cont-para-wrap'})\n",
    "    # for n in range(len(product_description)):\n",
    "    #   description += product_description.findAll('p')[n].text\n",
    "    # print(description)\n",
    "    description = \"\"\n",
    "    if len(product_description.findAll('p')) >= 1:\n",
    "      description = product_description.findAll('p')[0].text\n",
    "    else:\n",
    "      description = product_description.text\n",
    "\n",
    "    product_details = product_page_soup.findAll('div', {'class': 'v-prod-comp-dtls-listitem'})\n",
    "    brand = product_details[0].findAll('span')[1].text\n",
    "    dimensions = product_details[1].findAll('span')[1].text\n",
    "    weight = product_details[2].findAll('span')[1].text\n",
    "    warranty = product_details[3].findAll('span')[1].text\n",
    "    assembly = product_details[4].findAll('span')[1].text\n",
    "    fabric = product_details[5].findAll('span')[1].text\n",
    "\n",
    "\n",
    "    d = {\n",
    "        'price' : price,\n",
    "        'savings': savings,\n",
    "        'description': description,\n",
    "        'details' : {\n",
    "            'brand' : brand,\n",
    "            'dimensions': dimensions,\n",
    "            'weight': weight,\n",
    "            'warranty': warranty,\n",
    "            'assembly': assembly,\n",
    "            'primary material': fabric\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(dir_name,'metadata.txt'),'w') as f:\n",
    "        json.dump(d,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}